---
Author:
  - Ширяев Антон
tags:
  - embeddings
  - код
  - RAG
  - сравнительный_анализ
date: 2025-01-13
url:
imageNameKey:
---
# **Исследование и сравнительный анализ моделей векторных представлений (Embeddings) для семантического поиска по кодовой базе Python с комментариями на русском языке**

## **1\. Исполнительное резюме**

В настоящем отчете представлен исчерпывающий анализ ландшафта современных моделей для генерации векторных представлений (embeddings), применимых к задаче семантического поиска в рамках локальной инфраструктуры на базе Ollama. Исследование инициировано необходимостью определить оптимальную модель для индексации и поиска по кодовой базе на языке Python, содержащей документацию и комментарии на русском языке. Ключевым ограничением выступает аппаратная конфигурация, включающая графический ускоритель NVIDIA с объемом видеопамяти 16 ГБ (указанный как vVidia 5060ti, что интерпретируется как модель класса RTX 4060 Ti 16GB или аналогичная перспективная архитектура 50-й серии), а также требование локального запуска через среду Ollama.  
В качестве базовой модели для сравнения выбрана nomic-embed-text. Анализ, проведенный на основе последних данных бенчмарков MTEB, ruMTEB и CodeSearchNet, а также технической документации архитектур, показал, что на текущий момент существуют решения, значительно превосходящие nomic-embed-text по качеству семантического поиска в заданном контексте.  
Ключевые выводы исследования:

1. **Qwen3-Embedding** (версии 4B и 8B) является абсолютным лидером по качеству поиска, занимая первое место в глобальном рейтинге MTEB Multilingual и демонстрируя выдающиеся результаты в задачах поиска кода (Code Retrieval). Архитектура на базе декодера LLM позволяет модели глубоко понимать синтаксис Python и семантику русского языка, обеспечивая лучшее выравнивание между запросом и кодом.1  
2. **BGE-M3** представляет собой наиболее сбалансированное решение для систем, требующих поддержки гибридного поиска (плотные \+ разреженные векторы). Модель демонстрирует исключительные показатели на русскоязычных бенчмарках (ruMTEB) и эффективно справляется с длинными контекстами до 8192 токенов, что критично для анализа крупных файлов кода.4  
3. **Аппаратные возможности** в 16 ГБ видеопамяти позволяют отказаться от использования "малых" моделей (менее 1 млрд параметров) в пользу более мощных архитектур (4-8 млрд параметров), используя методы квантования (Q4\_K\_M), доступные в Ollama, без существенной потери качества.

Рекомендованной стратегией является переход на **Qwen3-Embedding-8B** (квантованная версия) или **BGE-M3**, в зависимости от требований к латентности системы.

## ---

**2\. Введение: Специфика семантического поиска по коду в многоязычной среде**

### **2.1 Эволюция задач Information Retrieval (IR)**

Традиционные системы информационного поиска долгое время опирались на лексические методы, такие как BM25 или TF-IDF, которые вычисляют релевантность на основе частоты встречаемости слов. Однако в контексте поиска по программному коду эти методы демонстрируют существенные ограничения. Пользовательский запрос часто формулируется на естественном языке и описывает *намерение* (например, "как распарсить конфиг"), тогда как целевой документ (код) содержит *реализацию* на формальном языке (например, import yaml; yaml.safe\_load()). Лексическое пересечение между запросом и кодом может быть минимальным или отсутствовать вовсе.  
Внедрение нейронных сетей и методов глубокого обучения привело к появлению плотного поиска (Dense Retrieval), где тексты преобразуются в низкоразмерные векторы в семантическом пространстве. Близость векторов (обычно косинусное сходство) отражает семантическую близость текстов. Для задачи поиска по коду это означает, что модель должна "понимать", что фраза "сортировка массива" семантически эквивалентна вызову функции .sort() или алгоритму быстрой сортировки, даже если само слово "сортировка" в коде не встречается.

### **2.2 Проблема кросс-лингвистического разрыва (Russian-Python Alignment)**

Исследуемая задача усложняется наличием двух лингвистических слоев:

1. **Естественный язык (Русский):** Комментарии, docstrings и запросы пользователей. Русский язык обладает богатой морфологией (падежи, склонения), что требует от модели качественного токенизатора и понимания контекста. Кроме того, технический русский язык насыщен англицизмами ("задеплоить", "смержить", "инстанс"), которые могут быть неизвестны моделям, обученным исключительно на литературных корпусах.  
2. **Формальный язык (Python/English):** Синтаксис Python базируется на английской лексике (def, class, return, raise). Переменные и функции также чаще всего именуются на английском (get\_user\_profile).

Эффективная модель эмбеддингов должна обладать способностью к кросс-лингвистическому выравниванию (cross-lingual alignment). Она должна проецировать вектор русского слова "список" и вектор английского термина list (или List из typing) в одну и ту же область многомерного пространства. Базовая модель nomic-embed-text, будучи обученной преимущественно на английских корпусах (хотя и имеющая некоторую многоязычность), может демонстрировать недостаточное качество этого выравнивания по сравнению с моделями, специально тренированными на массивных параллельных корпусах (как BGE-M3 или Qwen3).5

### **2.3 Роль среды Ollama и аппаратных ограничений**

Использование Ollama накладывает определенные требования к формату моделей (GGUF) и методам их квантования. Ollama предоставляет удобный интерфейс для локального запуска, абстрагируя сложность управления памятью, но требует совместимости архитектуры модели с бэкендом llama.cpp.  
Аппаратная конфигурация с 16 ГБ видеопамяти (VRAM) является ключевым фактором, определяющим пространство выбора.

* **Ограничение малых систем (4-8 ГБ VRAM):** Заставляет использовать модели класса BERT-Base (100-300 млн параметров), такие как all-minilm или nomic-embed-text.  
* **Возможности 16 ГБ VRAM:** Позволяют загружать модели класса LLM (до 7-8 млрд параметров в квантовании 4-bit или даже 8-bit). Это открывает доступ к моделям "нового поколения" (Generative Embeddings), которые обладают значительно более глубоким пониманием семантики и логики кода, чем классические энкодеры.7

## ---

**3\. Теоретические основы и архитектуры моделей эмбеддингов**

Для обоснованного выбора "лучшей" модели необходимо понимать архитектурные различия между кандидатами, представленными в экосистеме Ollama.

### **3.1 Двунаправленные энкодеры (Bi-Encoders: BERT, RoBERTa)**

Большинство традиционных моделей эмбеддингов, включая nomic-embed-text (v1/v1.5), mxbai-embed-large и BGE-M3, построены на архитектуре Трансформера-энкодера (по типу BERT).

* **Механизм работы:** Текст подается на вход модели целиком. Механизм внимания (Self-Attention) позволяет каждому токену взаимодействовать с каждым другим токеном в последовательности (двунаправленно). Финальный вектор (эмбеддинг) обычно берется из скрытого состояния специального токена \`\` или путем усреднения (mean pooling) всех выходных векторов.  
* **Преимущества:** Высокая скорость инференса (вывода), так как модель относительно компактна. Отлично подходят для задач классификации и поиска предложений.  
* **Недостатки для кода:** Ограниченная способность к "рассуждению" (reasoning). Модель запоминает паттерны, но с трудом выстраивает сложные логические связи, необходимые для понимания алгоритмов в коде. Также традиционные BERT-модели имеют ограничение контекста в 512 токенов (хотя Nomic и BGE расширили это до 8192).9

### **3.2 Эмбеддинги на базе декодеров (LLM-Based Embeddings)**

Новейший тренд 2024-2025 годов — использование больших языковых моделей (LLM), таких как Qwen или Mistral, в качестве эмбеддеров. Примером является серия **Qwen3-Embedding**.

* **Механизм работы:** Используется архитектура декодера (как в GPT). Текст обрабатывается авторегрессионно. Эмбеддинг извлекается из скрытого состояния последнего токена (часто \`\`). Поскольку LLM обучаются на гигантских объемах данных (триллионы токенов), включая весь GitHub и StackOverflow, они обладают "мировым знанием" о коде.  
* **Преимущества:**  
  1. **Инструктивность:** Модели обучены следовать инструкциям (Instruction Tuning). Можно явно указать модели: "Найди функцию Python, которая выполняет сортировку, описанную в запросе". Это позволяет динамически адаптировать векторное пространство под задачу.2  
  2. **Понимание кода:** Благодаря pre-training на коде, такие модели лучше понимают синтаксические конструкции Python.  
  3. **Кросс-лингвистичность:** LLM обучаются на смешанных данных, что дает им превосходные способности к переводу и сопоставлению понятий между русским и английским.3  
* **Недостатки:** Высокие требования к памяти и вычислительным ресурсам по сравнению с BERT-моделями. Однако 16 ГБ VRAM нивелируют этот недостаток для моделей до 8B параметров.

### **3.3 Архитектура Mixture of Experts (MoE)**

Модель **nomic-embed-text-v2-moe**, упомянутая в исследовании, использует архитектуру "Смесь экспертов".

* **Механизм работы:** Вместо одной гигантской нейросети используется набор специализированных подсетей ("экспертов"). Для каждого токена активируется только часть экспертов (например, 2 из 8).  
* **Преимущества:** Позволяет иметь большое общее количество параметров (емкость знаний) при низкой вычислительной стоимости инференса (активируется лишь часть параметров). Это обеспечивает высокую пропускную способность (tokens per second).10  
* **Релевантность:** Это попытка совместить качество больших моделей со скоростью малых. Для задач с высокой нагрузкой (тысячи запросов в секунду) это критично, но для локального поиска одного пользователя важнее может быть качество (Recall).

## ---

**4\. Глубокий анализ базовой модели: Nomic-embed-text**

Пользователь запрашивает сравнение с nomic-embed-text. Необходимо детально разобрать эту модель, чтобы понять "точку отсчета".

### **4.1 Характеристики**

* **Архитектура:** Основана на Nomic-BERT (модифицированный BERT с контекстом 2048, расширенный до 8192 через RoPE — Rotary Positional Embeddings).  
* **Версии:** Существует v1, v1.5 (наиболее популярная) и v2 (MoE). Базовая nomic-embed-text в Ollama обычно ссылается на v1.5.12  
* **Ключевая особенность:** Matryoshka Representation Learning. Модель обучена так, что информация в векторе упорядочена по важности. Можно отсечь конец вектора (например, использовать только первые 256 измерений из 768), и качество поиска упадет незначительно. Это экономит память в векторной базе данных (Vector DB).4

### **4.2 Ограничения в контексте задачи**

Несмотря на популярность, nomic-embed-text имеет недостатки для русскоязычного поиска по коду:

1. **Фокус обучения:** Обучающие данные v1.5 были сильно смещены в сторону английского языка и общих текстовых корпусов (Wikipedia, Papers). Специализированных русско-английских пар "код-комментарий" в обучении было меньше, чем у конкурентов.14  
2. **Качество на MTEB:** В бенчмарках MTEB модель показывает достойные, но не топовые результаты. В задачах поиска (Retrieval) она уступает более новым моделям серии BGE и E5, особенно на неанглийских языках.13  
3. **Обработка кириллицы:** Токенизатор Nomic оптимизирован под английский. Русские слова часто разбиваются на большее количество токенов, чем английские, что может "размывать" внимание модели при обработке длинных комментариев.

## ---

**5\. Анализ альтернативных моделей (Претенденты)**

На основе анализа доступных в Ollama моделей, выделяются три ключевых претендента, способных превзойти nomic-embed-text.

### **5.1 BGE-M3 (BAAI General Embedding)**

Модель BGE-M3 позиционируется как универсальное решение для многоязычного поиска.

* **Многоязычность (Multi-Linguality):** Модель явно обучалась на данных 100+ языков. Это критически важно для русского языка. Результаты бенчмарка **ruMTEB** (русский сегмент MTEB) показывают, что BGE-M3 является одним из лидеров в задачах Retrieval (поиск) и Reranking (переранжирование) среди моделей открытого доступа.5  
* **Гибридный поиск (Multi-Functionality):** BGE-M3 уникальна тем, что генерирует три типа выходов:  
  1. *Dense:* Классический вектор (как у Nomic).  
  2. *Sparse (Lexical):* Вектор, отражающий важность конкретных слов (аналог BM25, но обученный). Это позволяет находить код по точным совпадениям имен переменных или редких терминов в комментариях, где плотный вектор может "галлюцинировать".  
  3. *Multi-Vector (ColBERT):* Вектора для каждого токена для сверхточного переранжирования.4  
* **Применимость к коду:** Благодаря поддержке длинного контекста (8192 токена) и качественной поддержке русского языка, BGE-M3 отлично справляется с сопоставлением длинных описаний на русском с телом функций на Python.

### **5.2 Qwen3-Embedding (Серия)**

Это новейшая серия моделей от Alibaba Cloud, построенная на базе LLM Qwen.

* **SOTA показатели:** По состоянию на середину 2025 года (согласно данным из сниппетов), **Qwen3-Embedding-8B** занимает первое место в рейтинге MTEB Multilingual с баллом **70.58**, значительно опережая предыдущих лидеров. Для сравнения, средний балл Nomic и BGE-M3 находится в диапазоне 62-65.1  
* **Понимание кода:** Семейство моделей Qwen (особенно ветка Qwen-Coder) специально тренируется на огромных массивах кода. Это дает модели "интуитивное" понимание структуры Python, которого нет у чисто текстовых моделей типа Nomic.  
* **Инструктивность:** Модель требует использования промптов (инструкций).  
  * *Запрос:* Instruct: Given a web search query, retrieve relevant code snippets. Query: как отфильтровать список словарей  
  * Код: Без инструкции или с инструкцией Represent this code snippet.  
    Такой подход позволяет "настроить" модель именно на поиск кода, отсекая шум общеязыковых ассоциаций.15  
* **Аппаратная совместимость:** Модель 8B в квантовании Q4\_K\_M занимает около 5-6 ГБ VRAM, модель 4B (FP16) — около 8 ГБ. Обе версии легко помещаются в 16 ГБ VRAM NVIDIA 5060Ti/4060Ti, оставляя место для контекста.

### **5.3 Nomic-embed-text-v2-moe**

Это эволюционное развитие базовой модели пользователя.

* **MoE Архитектура:** Обеспечивает высокую скорость работы.  
* **Улучшенная многоязычность:** Заявлена поддержка \~100 языков и обучение на 1.6 млрд пар текстов. Это должно решить проблему слабой поддержки русского языка в v1.5.10  
* **Проблемы интеграции:** На момент написания отчета (анализ сниппетов показывает обсуждения 2025 года) поддержка MoE архитектур в GGUF/Ollama может требовать свежих версий и специфических настроек префиксов (search\_query:, search\_document:). Если эти префиксы не добавить, качество модели падает катастрофически, так как маршрутизация токенов к экспертам сбивается.16

## ---

**6\. Сравнительный анализ производительности (Benchmarking)**

Для объективного сравнения необходимо обратиться к результатам стандартизированных тестов. Мы рассмотрим три измерения: MTEB (общий), ruMTEB (русский язык) и Code Search (поиск кода).

### **6.1 Таблица сравнительных характеристик**

| Характеристика           | nomic-embed-text (v1.5) | BGE-M3                | Qwen3-Embedding-8B         | Qwen3-Embedding-0.6B |
| :----------------------- | :---------------------- | :-------------------- | :------------------------- | :------------------- |
| **Тип архитектуры**      | BERT (Encoder)          | BERT (Encoder)        | LLM (Decoder)              | LLM (Decoder)        |
| **Размер (Параметры)**   | 137M                    | 567M                  | 8B                         | 0.6B                 |
| **Контекстное окно**     | 8192                    | 8192                  | **32,000**                 | **32,000**           |
| **MTEB Multilingual**    | \~61.0                  | 64.2                  | **70.58 (\#1)**            | 64.33                |
| **Code Retrieval Score** | Средний                 | Высокий               | **Очень высокий (\~80.6)** | Высокий              |
| **Поддержка русского**   | Базовая                 | **Отличная (ruMTEB)** | **Превосходная**           | Хорошая              |
| **VRAM (FP16/Q4)**       | \< 1 ГБ                 | \~1.2 ГБ              | \~16 ГБ / \~5.5 ГБ         | \~1.5 ГБ / \~0.5 ГБ  |
| **Требует инструкций?**  | Нет (опционально)       | Нет                   | **Да (Критично)**          | **Да**               |

**Анализ данных:**

1. **Доминирование Qwen3:** Данные показывают, что модель **Qwen3-Embedding-8B** является абсолютным лидером. Даже ее "маленькая" версия (0.6B) превосходит или равна по качеству BGE-M3 и Nomic v1.5 на многоязычных задачах.3  
2. **Контекст:** Qwen3 предлагает контекст в **32,000 токенов**.17 Это колоссальное преимущество для поиска по коду. Файл с исходным кодом Python может быть очень длинным. Nomic и BGE ограничены 8k токенами, что много, но 32k позволяют индексировать целые модули или очень подробную документацию без необходимости агрессивного разбиения на чанки (chunking), которое часто разрывает контекст.  
3. **Русский язык (ruMTEB):** Исследования показывают, что BGE-M3 долгое время удерживала лидерство в русском сегменте благодаря специальному выравниванию.6 Однако архитектура LLM в Qwen3, обученная на большем объеме данных, демонстрирует лучшую генерализацию.

### **6.2 Интерпретация результатов для задачи пользователя**

Для задачи "поиск Python кода по запросу с русскими комментариями" критически важна способность модели связывать разнородные понятия.

* *Пример:* Запрос "функция для парсинга аргументов командной строки".  
* *Код:* parser \= argparse.ArgumentParser()  
* *Nomic:* Может найти этот код, если в документации рядом есть слово "parse".  
* *Qwen3:* С высокой вероятностью найдет код, даже если комментариев нет, так как "знает", что argparse используется для "парсинга аргументов" (семантическая связь, выученная на этапе pre-training). Если комментарии есть ("\# парсим аргументы"), связь усиливается.

## ---

**7\. Аппаратная оптимизация и использование ресурсов (NVIDIA 16GB)**

Наличие 16 ГБ видеопамяти — это роскошь для задач эмбеддинга, позволяющая использовать модели, недоступные большинству пользователей с картами 8 ГБ.

### **7.1 Стратегия использования памяти**

Видеопамять расходуется на:

1. **Веса модели:** Зависит от размера и квантования.  
2. **KV-Cache (Контекст):** Зависит от длины контекста и batch size (размера пачки).  
3. **Активации:** Временные данные при прогоне нейросети.

**Расчет для Qwen3-Embedding-8B (Q4\_K\_M):**

* Вес файла GGUF: \~5.0 \- 5.5 ГБ.  
* Оверхед Ollama (CUDA context): \~0.5 \- 1 ГБ.  
* **Итого:** \~6.5 ГБ статически.  
* **Остаток:** \~9.5 ГБ свободно для обработки длинных контекстов и больших батчей.

**Вывод:** Пользователь может комфортно запускать **Qwen3-Embedding-8B** в квантовании 4-bit или даже 6-bit (Q6\_K), получая качество SOTA-модели при высокой скорости. Нет необходимости ограничивать себя моделями класса 300M (Nomic/BGE) ради экономии памяти.

### **7.2 Квантование и его влияние**

Исследования показывают, что для эмбеддинг-моделей квантование до 4 бит (Q4\_K\_M) оказывает минимальное влияние на качество поиска (снижение метрики NDCG@10 обычно менее 1-2%), но ускоряет работу и снижает потребление памяти в 2-3 раза. Для 8B модели это оптимальный компромисс.

## ---

**8\. Практическая реализация в Ollama**

Для успешного внедрения выбранных моделей необходимо учитывать специфику их работы в Ollama.

### **8.1 Запуск Qwen3-Embedding**

Поскольку Qwen3 требует инструкций, стандартный вызов ollama.embeddings() может работать некорректно, если не добавить префикс к тексту запроса.  
**Рекомендуемый алгоритм:**

1. Загрузка модели: ollama pull qwen3-embedding:8b (проверьте точное название тега в библиотеке Ollama, так как новые модели добавляются быстро). Если тега нет, можно импортировать GGUF файл вручную через Modelfile.  
2. Формирование запроса:  
   При отправке текста на эмбеддинг, программно модифицируйте строку:  
   * *Было:* "как прочитать csv"  
   * Стало: "Instruct: Given a web search query, retrieve relevant code snippets. Query: как прочитать csv"  
     (Точный текст инструкции лучше брать из карточки модели на HuggingFace, так как он может варьироваться).  
3. **Индексация кода:** Для документов инструкция обычно не требуется или пустая.

### **8.2 Запуск BGE-M3**

Модель BGE-M3 работает "из коробки" проще.

1. Загрузка: ollama pull bge-m3  
2. Использование: Префиксы не обязательны, но BGE поддерживает инструкции. Если результаты неудовлетворительны, можно попробовать добавить инструкцию для асимметричного поиска.

### **8.3 Работа с Modelfile для Nomic v2**

Если пользователь решит попробовать nomic-embed-text-v2-moe, крайне важно использовать правильные системные параметры в Modelfile для указания токенизатора и префиксов, иначе MoE-роутинг будет работать со случайным шумом.

## ---

**9\. Синтез и Рекомендации**

На основе проведенного анализа, можно с уверенностью утверждать, что **модели, превосходящие nomic-embed-text, существуют и доступны для запуска на указанном оборудовании.**

### **9.1 Сравнительная иерархия решений**

**1\. Абсолютный лидер (The Best Choice): Qwen3-Embedding-8B (или 4B)**

* **Почему:** Лучшее понимание кода, лучшее понимание связи "Русский-Код", гигантское контекстное окно (32k), SOTA метрики.  
* **Для 16GB VRAM:** Рекомендуется использовать версию **8B** в квантовании Q4\_K\_M. Если скорость критична, версия **4B** обеспечит качество выше Nomic при очень высокой скорости.  
* **Действие:** Использовать этот вариант как основной.

**2\. Лучшая альтернатива (The Robust Choice): BGE-M3**

* **Почему:** Проверенная временем надежность в многоязычных задачах, отличные результаты на русском языке, поддержка гибридного поиска (если пользователь будет использовать Sparse vectors в будущем).  
* **Действие:** Использовать, если Qwen3 окажется слишком тяжелым или сложным в интеграции промптов.

**3\. Эффективный апгрейд: Nomic-embed-text-v2-moe**

* **Почему:** Лучше, чем v1.5, но уступает Qwen3 в "интеллекте". Хороший выбор, если нужно сохранить экосистему Nomic.

### **9.2 Итоговая рекомендация**

Для получения наилучших эмбеддингов для семантического поиска по базе кода Python с русскими комментариями на GPU NVIDIA 16GB, **рекомендуется перейти с nomic-embed-text на qwen3-embedding (версии 8b или 4b)**.  
Эта модель обеспечит:

1. Более точное нахождение кода по смыслу, а не по ключевым словам.  
2. Корректную обработку смешанных русско-английских запросов.  
3. Возможность индексировать крупные файлы целиком благодаря окну в 32k токенов.

Пользователю следует убедиться, что его программное обеспечение (клиент Ollama, LangChain, LlamaIndex) поддерживает добавление инструкций (prompts) к запросам перед генерацией эмбеддинга, так как это критично для производительности моделей Qwen.

## ---

**Приложение: Таблицы данных**

### **Таблица 1\. Сравнение моделей по ключевым метрикам**

| Модель                 | Параметры | MTEB Score (Multi) | Code Retrieval | Контекст | VRAM (Q4/FP16)  | Рекомендация  |
| :--------------------- | :-------- | :----------------- | :------------- | :------- | :-------------- | :------------ |
| **Qwen3-Embedding-8B** | 8B        | **70.58**          | **SOTA**       | 32,000   | \~5.5GB / 16GB  | **Высокая**   |
| **Qwen3-Embedding-4B** | 4B        | 69.45              | Высокий        | 32,000   | \~3.0GB / 8GB   | **Высокая**   |
| **BGE-M3**             | 0.6B      | 64.20              | Высокий        | 8,192    | \~0.5GB / 1.2GB | **Средняя**   |
| **nomic-embed-text**   | 0.14B     | \~61.0             | Средний        | 8,192    | \<0.5GB         | Низкая (База) |

### **Таблица 2\. Пример использования инструкций для Qwen3**

| Тип данных | Шаблон инструкции (Prompt) |
| :---- | :---- |
| **Пользовательский запрос** | Instruct: Given a web search query, retrieve relevant code snippets.\\nQuery: {текст\_запроса} |
| **Документ (Код)** | *(Обычно без инструкции)* или Instruct: Represent this code for retrieval.\\nCode: {код} |

### **Ссылочные идентификаторы (Citations)**

.1

#### **Источники**

1. QwenLM/Qwen3-Embedding \- GitHub, дата последнего обращения: января 13, 2026, [https://github.com/QwenLM/Qwen3-Embedding](https://github.com/QwenLM/Qwen3-Embedding)  
2. Qwen/Qwen3-Embedding-8B \- Hugging Face, дата последнего обращения: января 13, 2026, [https://huggingface.co/Qwen/Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)  
3. Qwen3 Embedding: Advancing Text Embedding and Reranking ..., дата последнего обращения: января 13, 2026, [https://arxiv.org/pdf/2506.05176](https://arxiv.org/pdf/2506.05176)  
4. The Best Open-Source Embedding Models in 2026 \- BentoML, дата последнего обращения: января 13, 2026, [https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models](https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models)  
5. ruMTEB benchmark and Russian embedding model design, дата последнего обращения: января 13, 2026, [https://aclanthology.org/2025.naacl-long.12.pdf](https://aclanthology.org/2025.naacl-long.12.pdf)  
6. ruMTEB benchmark and Russian embedding model design \- Liner, дата последнего обращения: января 13, 2026, [https://liner.com/review/russianfocused-embedders-exploration-rumteb-benchmark-and-russian-embedding-model-design](https://liner.com/review/russianfocused-embedders-exploration-rumteb-benchmark-and-russian-embedding-model-design)  
7. Choosing the Best Ollama Model for Your Coding Projects \- CodeGPT, дата последнего обращения: января 13, 2026, [https://www.codegpt.co/blog/choosing-best-ollama-model](https://www.codegpt.co/blog/choosing-best-ollama-model)  
8. ollama/ollama: Get up and running with OpenAI gpt-oss ... \- GitHub, дата последнего обращения: января 13, 2026, [https://github.com/ollama/ollama](https://github.com/ollama/ollama)  
9. Nomic Embeddings — A cheaper and better way to create ... \- Medium, дата последнего обращения: января 13, 2026, [https://medium.com/@guptak650/nomic-embeddings-a-cheaper-and-better-way-to-create-embeddings-6590868b438f](https://medium.com/@guptak650/nomic-embeddings-a-cheaper-and-better-way-to-create-embeddings-6590868b438f)  
10. nomic-embed-text-v2-moe \- Ollama, дата последнего обращения: января 13, 2026, [https://ollama.com/library/nomic-embed-text-v2-moe](https://ollama.com/library/nomic-embed-text-v2-moe)  
11. toshk0/nomic-embed-text-v2-moe:Q6\_K \- Ollama, дата последнего обращения: января 13, 2026, [https://ollama.com/toshk0/nomic-embed-text-v2-moe:Q6\_K](https://ollama.com/toshk0/nomic-embed-text-v2-moe:Q6_K)  
12. Embedding models · Ollama Search, дата последнего обращения: января 13, 2026, [https://ollama.com/search?c=embedding](https://ollama.com/search?c=embedding)  
13. Finding the Best Open-Source Embedding Model for RAG \- Tiger Data, дата последнего обращения: января 13, 2026, [https://www.tigerdata.com/blog/finding-the-best-open-source-embedding-model-for-rag](https://www.tigerdata.com/blog/finding-the-best-open-source-embedding-model-for-rag)  
14. NV-Embed vs BGE-M3 vs Nomic: Picking the Right Embeddings for ..., дата последнего обращения: января 13, 2026, [https://ai-marketinglabs.com/lab-experiments/nv-embed-vs-bge-m3-vs-nomic-picking-the-right-embeddings-for-pinecone-rag](https://ai-marketinglabs.com/lab-experiments/nv-embed-vs-bge-m3-vs-nomic-picking-the-right-embeddings-for-pinecone-rag)  
15. Mastering Text Embedding and Reranker with Qwen3 \- Alibaba Cloud, дата последнего обращения: января 13, 2026, [https://www.alibabacloud.com/blog/mastering-text-embedding-and-reranker-with-qwen3\_602308](https://www.alibabacloud.com/blog/mastering-text-embedding-and-reranker-with-qwen3_602308)  
16. toshk0/nomic-embed-text-v2-moe \- Ollama, дата последнего обращения: января 13, 2026, [https://ollama.com/toshk0/nomic-embed-text-v2-moe](https://ollama.com/toshk0/nomic-embed-text-v2-moe)  
17. Qwen3 Embedding 0.6B Free Chat Online \- Skywork.ai, дата последнего обращения: января 13, 2026, [https://skywork.ai/blog/models/qwen3-embedding-0-6b-free-chat-online/](https://skywork.ai/blog/models/qwen3-embedding-0-6b-free-chat-online/)  
18. MTEB Leaderboard \- a Hugging Face Space by mteb, дата последнего обращения: января 13, 2026, [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)  
19. Comparative Analysis of Qwen-3 and BGE-M3 Embedding Models ..., дата последнего обращения: января 13, 2026, [https://medium.com/@mrAryanKumar/comparative-analysis-of-qwen-3-and-bge-m3-embedding-models-for-multilingual-information-retrieval-72c0e6895413](https://medium.com/@mrAryanKumar/comparative-analysis-of-qwen-3-and-bge-m3-embedding-models-for-multilingual-information-retrieval-72c0e6895413)  
20. QwenLM/Qwen3-VL-Embedding \- GitHub, дата последнего обращения: января 13, 2026, [https://github.com/QwenLM/Qwen3-VL-Embedding](https://github.com/QwenLM/Qwen3-VL-Embedding)  
21. nomic-ai/nomic-embed-text-v2-moe-GGUF \- Hugging Face, дата последнего обращения: января 13, 2026, [https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe-GGUF](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe-GGUF)  
22. Qwen/Qwen3-Embedding-0.6B \- Hugging Face, дата последнего обращения: января 13, 2026, [https://huggingface.co/Qwen/Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)